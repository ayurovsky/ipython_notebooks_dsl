{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "#with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "\n",
    "#with codecs.open(\"Diwan_MissingArchaea_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    " #   with open('Diwan_MissingArchaea_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "with codecs.open(\"Diwan_MissingBacteria_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('Diwan_MissingBacteria_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "        \n",
    "with codecs.open(\"DiwanDataFilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('DiwanDataFilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff - just run the processing on the 48 files not found in bacteria\n",
    "# on the previous run\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_archaea_db/archaea_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingArchaea_FilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "#with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_bacteria_db/general_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        #if (name in species_dict_by_name):\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingBacteria_FilesInfo.csv', 'w') as f:\n",
    "#with open('DiwanDataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on Ruhul's 153 files\n",
    "with open('Ruhul_Annotate_153_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    with codecs.open(\"ruhul_re-annotation_all_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000191585.1_ASM19158v1_genomic.gff.gz']\n",
      "GCA_000191585.1\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000306765.2_ASM30676v2_genomic.gff.gz']\n",
      "GCA_000306765.2\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000017185.1_ASM1718v1_genomic.gff.gz']\n",
      "GCA_000017185.1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on original files from the paper\n",
    "\n",
    "#with open('Our_NOT_IN_DIWAN_SD_Bacteria_DataFilesInfo.csv', 'w') as f:\n",
    "with open('Our_NOT_IN_DIWAN_SD_Archaea_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    #with codecs.open(\"our_from_paper_not_in_diwan_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"our_from_paper_not_in_diwan_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            #names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                print(names)\n",
    "                print(prefix)\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, name missing from both bacteria and archaea:  full_file_prefix\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the 166 bad tompa entries to separate archaea from bacteria\n",
    "\n",
    "#with open('166_bad_tompa_Bacteria_DataFilesInfo.csv', 'w') as f_bacteria:\n",
    " #   with open('166_bad_tompa_Archaea_DataFilesInfo.csv', 'w') as f_archaea:\n",
    "with open('all_just_tompa_Bacteria_DataFilesInfo.csv', 'w') as f_bacteria:\n",
    "    with open('all_just_tompa_Archaea_DataFilesInfo.csv', 'w') as f_archaea:\n",
    "        csvWriter_bacteria = csv.writer(f_bacteria, delimiter=',')\n",
    "        csvWriter_bacteria.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "        csvWriter_archaea = csv.writer(f_archaea, delimiter=',')\n",
    "        csvWriter_archaea.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "        # read the large excel file, and find the correct genome\n",
    "        #with codecs.open(\"our_from_paper_not_in_diwan_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        #with codecs.open(\"166_bad_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        with codecs.open(\"all_just_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "            for row in f:\n",
    "                data = row.strip().split(\",\")\n",
    "                name = data[0]\n",
    "                full_prefix = data[1]\n",
    "                end_of_16S = data[2]\n",
    "                names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + full_prefix + '*.gz')\n",
    "                if (len(names) > 1):\n",
    "                    print(\"Error, in Bacteria, more than 1 file with prefix: \", full_prefix)\n",
    "                else:\n",
    "                    if (len(names) > 0):\n",
    "                        csvWriter_bacteria.writerow(data) #[name, full_prefix, end_of_16S, \"\"])       \n",
    "                    else:\n",
    "                        names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + full_prefix + '*.gz')\n",
    "                        if (len(names) > 1):\n",
    "                            print(\"Error, in Archaea, more than 1 file with prefix: \", full_prefix)\n",
    "                        else:\n",
    "                            if (len(names) > 0):\n",
    "                                csvWriter_archaea.writerow(data) #[name, full_prefix, end_of_16S, \"\"])\n",
    "                            else:\n",
    "                                print(\"Error, name missing from both bacteria and archaea: \", full_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post-processing - is SD that is found in the tail?\n",
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run pipeline on our stuff\n",
    "\n",
    "\n",
    "#with open('DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "#with open('Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153_rounded_annotated.csv', 'w') as f:\n",
    "#with open('Diwan_Missing_48_allstats_rounded_annotated.csv', 'w') as f:\n",
    "#with open('166_missing_DataFiles_WithAllStats_only_tompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "with open('all_just_tompa_WithAllStats_withTompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    # read the large excel file, and find the correct genome\n",
    "    #with codecs.open(\"Diwan_Missing_48_allstats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    #with codecs.open(\"Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "    #with codecs.open(\"DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    #with codecs.open(\"166_missing_allstats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"all_just_tompa_WithAllStats_withTompa_v4.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data + [\"found in 16S\"])\n",
    "                continue\n",
    "            seq = data[3]\n",
    "            complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "            reverse_complement = \"\".join(complement.get(base, base) for base in reversed(seq))\n",
    "            #print(reverse_complement)\n",
    "            if ((\"-\" not in seq) and (\"-\" not in data[2]) and (reverse_complement in data[2])):\n",
    "                csvWriter.writerow(data + [\"yes\"])\n",
    "            else:\n",
    "                csvWriter.writerow(data + [\"no\"])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.43\n",
      "_____\n",
      "10.83\n",
      "_____\n",
      "19.46\n",
      "_____\n",
      "17.01\n",
      "_____\n",
      "10.22\n",
      "_____\n",
      "12.71\n",
      "_____\n",
      "19.62\n",
      "_____\n",
      "17.97\n",
      "_____\n",
      "19\n",
      "_____\n",
      "17.24\n",
      "_____\n",
      "16.31\n",
      "_____\n",
      "8.853\n",
      "_____\n",
      "14.24\n",
      "_____\n",
      "15.66\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "7.009\n",
      "_____\n",
      "14.18\n",
      "_____\n",
      "18.52\n",
      "_____\n",
      "10.75\n",
      "_____\n",
      "22.9\n",
      "_____\n",
      "24.89\n",
      "_____\n",
      "11.93\n",
      "_____\n",
      "19.12\n",
      "_____\n",
      "14.07\n",
      "_____\n",
      "19.96\n",
      "_____\n",
      "6.136\n",
      "_____\n",
      "15.93\n",
      "_____\n",
      "16.06\n",
      "_____\n",
      "13.22\n",
      "_____\n",
      "10.09\n",
      "_____\n",
      "20.24\n",
      "_____\n",
      "13.97\n",
      "_____\n",
      "17.38\n",
      "_____\n",
      "73.98\n",
      "_____\n",
      "62.99\n",
      "_____\n",
      "50.73\n",
      "_____\n",
      "22.09\n",
      "_____\n",
      "21.86\n",
      "_____\n",
      "16.28\n",
      "_____\n",
      "22.8\n",
      "_____\n",
      "50.21\n",
      "_____\n",
      "10.38\n",
      "_____\n",
      "10.64\n",
      "_____\n",
      "17.47\n",
      "_____\n",
      "16.42\n",
      "_____\n",
      "25.06\n",
      "_____\n",
      "22.9\n",
      "_____\n",
      "16.84\n",
      "_____\n",
      "31.65\n",
      "_____\n",
      "7.115\n",
      "_____\n",
      "8.609\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "36.51\n",
      "_____\n",
      "19.31\n",
      "_____\n",
      "28.07\n",
      "_____\n",
      "30.4\n",
      "_____\n",
      "64.23\n",
      "_____\n",
      "8.114\n",
      "_____\n",
      "15.06\n",
      "_____\n",
      "64.61\n",
      "_____\n",
      "49.19\n",
      "_____\n",
      "19.73\n",
      "_____\n",
      "53.33\n",
      "_____\n",
      "71.43\n",
      "_____\n",
      "66.32\n",
      "_____\n",
      "68.28\n",
      "_____\n",
      "28.19\n",
      "_____\n",
      "67.24\n",
      "_____\n",
      "73.58\n",
      "_____\n",
      "15.36\n",
      "_____\n",
      "7.831\n",
      "_____\n",
      "31.45\n",
      "_____\n",
      "20.43\n",
      "_____\n",
      "30.54\n",
      "_____\n",
      "8.775\n",
      "_____\n",
      "5.702\n",
      "_____\n",
      "11.93\n",
      "_____\n",
      "7.975\n",
      "_____\n",
      "25.77\n",
      "_____\n",
      "22.01\n",
      "_____\n",
      "15.58\n",
      "_____\n",
      "11.67\n",
      "_____\n",
      "28.19\n",
      "_____\n",
      "13.93\n",
      "_____\n",
      "27.27\n",
      "_____\n",
      "20.64\n",
      "_____\n",
      "18.83\n",
      "_____\n",
      "29.64\n",
      "_____\n",
      "43.32\n",
      "_____\n",
      "53.27\n",
      "_____\n",
      "21.73\n",
      "_____\n",
      "43.65\n",
      "_____\n",
      "43.11\n",
      "_____\n",
      "7.235\n",
      "_____\n",
      "7.281\n",
      "_____\n",
      "9.245\n",
      "_____\n",
      "19.42\n",
      "_____\n",
      "23.75\n",
      "_____\n",
      "22.12\n",
      "_____\n",
      "19.82\n",
      "_____\n",
      "17.33\n",
      "_____\n",
      "20.12\n",
      "_____\n",
      "25.97\n",
      "_____\n",
      "14.51\n",
      "_____\n",
      "9.918\n",
      "_____\n",
      "43.05\n",
      "_____\n",
      "9.078\n",
      "_____\n",
      "6.145\n",
      "_____\n",
      "7.137\n",
      "_____\n",
      "6.422\n",
      "_____\n",
      "37.73\n",
      "_____\n",
      "11.8\n",
      "_____\n",
      "10.17\n",
      "_____\n",
      "6.391\n",
      "_____\n",
      "9.657\n",
      "_____\n",
      "11.14\n",
      "_____\n",
      "17.98\n",
      "_____\n",
      "18.28\n",
      "_____\n",
      "36.7\n",
      "_____\n",
      "20.79\n",
      "_____\n",
      "8.241\n",
      "_____\n",
      "8.066\n",
      "_____\n",
      "15.17\n",
      "_____\n",
      "15.17\n",
      "_____\n",
      "20.41\n",
      "_____\n",
      "22.55\n",
      "_____\n",
      "22.44\n",
      "_____\n",
      "22.21\n",
      "_____\n",
      "45.85\n",
      "_____\n",
      "8.17\n",
      "_____\n",
      "58.79\n",
      "_____\n",
      "45.66\n",
      "_____\n",
      "47.72\n",
      "_____\n",
      "17.1\n",
      "_____\n",
      "18.01\n",
      "_____\n",
      "23.94\n",
      "_____\n",
      "18.5\n",
      "_____\n",
      "17.72\n",
      "_____\n",
      "14.04\n",
      "_____\n",
      "27.58\n",
      "_____\n",
      "25.93\n",
      "_____\n",
      "28.27\n",
      "_____\n",
      "22.12\n",
      "_____\n",
      "40.56\n",
      "_____\n",
      "24.71\n",
      "_____\n",
      "45.11\n",
      "_____\n",
      "23.55\n",
      "_____\n",
      "23.46\n",
      "_____\n",
      "29.36\n",
      "_____\n",
      "9.438\n",
      "_____\n",
      "32.43\n",
      "_____\n",
      "27.42\n",
      "_____\n",
      "30.24\n",
      "_____\n",
      "10.77\n",
      "_____\n",
      "10.12\n",
      "_____\n",
      "60.08\n",
      "_____\n",
      "47.11\n",
      "_____\n",
      "41.31\n",
      "_____\n",
      "8.617\n",
      "_____\n",
      "32.24\n",
      "_____\n",
      "15.06\n",
      "_____\n",
      "21.3\n",
      "_____\n",
      "16.12\n",
      "_____\n",
      "12.32\n",
      "_____\n",
      "61.46\n",
      "_____\n",
      "58.58\n",
      "_____\n",
      "18.05\n",
      "_____\n",
      "21.18\n",
      "_____\n",
      "15.12\n",
      "_____\n",
      "7.735\n",
      "_____\n",
      "12.37\n",
      "_____\n",
      "21.5\n",
      "_____\n",
      "20.39\n",
      "_____\n",
      "19.94\n",
      "_____\n",
      "19.6\n",
      "_____\n",
      "14.72\n",
      "_____\n",
      "37.99\n",
      "_____\n",
      "17.76\n",
      "_____\n",
      "18.19\n",
      "_____\n",
      "10.11\n",
      "_____\n",
      "21.9\n",
      "_____\n",
      "19.03\n",
      "_____\n",
      "13.02\n",
      "_____\n",
      "14.79\n",
      "_____\n",
      "5.676\n",
      "_____\n",
      "9.041\n",
      "_____\n",
      "5.409\n",
      "_____\n",
      "5.887\n",
      "_____\n",
      "6.734\n",
      "_____\n",
      "6.734\n",
      "_____\n",
      "8.726\n",
      "_____\n",
      "14.2\n",
      "_____\n",
      "12.64\n",
      "_____\n",
      "12.05\n",
      "_____\n",
      "7.468\n",
      "_____\n",
      "9.861\n",
      "_____\n",
      "6.973\n",
      "_____\n",
      "11.59\n",
      "_____\n",
      "8.371\n",
      "_____\n",
      "9.011\n",
      "_____\n",
      "9.011\n",
      "_____\n",
      "7.686\n",
      "_____\n",
      "12.06\n",
      "_____\n",
      "27.95\n",
      "_____\n",
      "30.39\n",
      "_____\n",
      "33.81\n",
      "_____\n",
      "6.01\n",
      "_____\n",
      "15.12\n",
      "_____\n",
      "13.73\n",
      "_____\n",
      "19.59\n",
      "_____\n",
      "21.23\n",
      "_____\n",
      "18.36\n",
      "_____\n",
      "21.5\n",
      "_____\n",
      "30.46\n",
      "_____\n",
      "15.65\n",
      "_____\n",
      "18.42\n",
      "_____\n",
      "64.74\n",
      "_____\n",
      "32.82\n",
      "_____\n",
      "22.2\n",
      "_____\n",
      "5.053\n",
      "_____\n",
      "22.41\n",
      "_____\n",
      "7.16\n",
      "_____\n",
      "10.3\n",
      "_____\n",
      "6.601\n",
      "_____\n",
      "5.954\n",
      "_____\n",
      "6.379\n",
      "_____\n",
      "7.804\n",
      "_____\n",
      "24.48\n",
      "_____\n",
      "27.94\n",
      "_____\n",
      "37.69\n",
      "_____\n",
      "79\n",
      "_____\n",
      "15.11\n",
      "_____\n",
      "26.3\n",
      "_____\n",
      "30.71\n",
      "_____\n",
      "11.23\n",
      "_____\n",
      "7.992\n",
      "_____\n",
      "27.81\n",
      "_____\n",
      "18.96\n",
      "_____\n",
      "18.43\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "18.96\n",
      "_____\n",
      "40.15\n",
      "_____\n",
      "35.43\n",
      "_____\n",
      "31.11\n",
      "_____\n",
      "29.51\n",
      "_____\n",
      "25.27\n",
      "_____\n",
      "28.34\n",
      "_____\n",
      "33.51\n",
      "_____\n",
      "27.38\n",
      "_____\n",
      "23.79\n",
      "_____\n",
      "23.51\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "31.55\n",
      "_____\n",
      "23.9\n",
      "_____\n",
      "13.28\n",
      "_____\n",
      "17.65\n",
      "_____\n",
      "25.63\n",
      "_____\n",
      "20.88\n",
      "_____\n",
      "13.27\n",
      "_____\n",
      "20.6\n",
      "_____\n",
      "9.333\n",
      "_____\n",
      "8.3\n",
      "_____\n",
      "9.238\n",
      "_____\n",
      "26.07\n",
      "_____\n",
      "24.8\n",
      "_____\n",
      "23.15\n",
      "_____\n",
      "22.34\n",
      "_____\n",
      "22.44\n",
      "_____\n",
      "25.38\n",
      "_____\n",
      "27.04\n",
      "_____\n",
      "24.85\n",
      "_____\n",
      "34.4\n",
      "_____\n",
      "20.66\n",
      "_____\n",
      "22.39\n",
      "_____\n",
      "47.24\n",
      "_____\n",
      "19.52\n",
      "_____\n",
      "21.86\n",
      "_____\n",
      "35.24\n",
      "_____\n",
      "6.944\n",
      "_____\n",
      "38.4\n",
      "_____\n",
      "22.25\n",
      "_____\n",
      "26.77\n",
      "_____\n",
      "24.37\n",
      "_____\n",
      "25.87\n",
      "_____\n",
      "19.65\n",
      "_____\n",
      "21.8\n",
      "_____\n",
      "21.32\n",
      "_____\n",
      "60.28\n",
      "_____\n",
      "57.21\n",
      "_____\n",
      "62.33\n",
      "_____\n",
      "62.45\n",
      "_____\n",
      "40.6\n",
      "_____\n",
      "40.81\n",
      "_____\n",
      "39.17\n",
      "_____\n",
      "30.93\n",
      "_____\n",
      "41.78\n",
      "_____\n",
      "27.35\n",
      "_____\n",
      "32.16\n",
      "_____\n",
      "36.63\n",
      "_____\n",
      "19.68\n",
      "_____\n",
      "48.44\n",
      "_____\n",
      "36.08\n",
      "_____\n",
      "11.33\n",
      "_____\n",
      "21.96\n",
      "_____\n",
      "44.56\n",
      "_____\n",
      "27.39\n",
      "_____\n",
      "21.12\n",
      "_____\n",
      "13.58\n",
      "_____\n",
      "48.05\n",
      "_____\n",
      "37.67\n",
      "_____\n",
      "40.87\n",
      "_____\n",
      "42.58\n",
      "_____\n",
      "24.65\n",
      "_____\n",
      "18.56\n",
      "_____\n",
      "39.37\n",
      "_____\n",
      "16.21\n",
      "_____\n",
      "20.14\n",
      "_____\n",
      "6.802\n",
      "_____\n",
      "10.91\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "16.25\n",
      "_____\n",
      "17.78\n",
      "_____\n",
      "18.2\n",
      "_____\n",
      "20.01\n",
      "_____\n",
      "20.01\n",
      "_____\n",
      "13.31\n",
      "_____\n",
      "5.694\n",
      "_____\n",
      "35.81\n",
      "_____\n",
      "19.16\n",
      "_____\n",
      "20.52\n",
      "_____\n",
      "18.85\n",
      "_____\n",
      "13.9\n",
      "_____\n",
      "25.63\n",
      "_____\n",
      "26.21\n",
      "_____\n",
      "12.58\n",
      "_____\n",
      "13.16\n",
      "_____\n",
      "16.68\n",
      "_____\n",
      "29.25\n",
      "_____\n",
      "7.715\n",
      "_____\n",
      "8.624\n",
      "_____\n",
      "8.412\n",
      "_____\n",
      "6.625\n",
      "_____\n",
      "6.7\n",
      "_____\n",
      "8.492\n",
      "_____\n",
      "7.262\n",
      "_____\n",
      "28.74\n",
      "_____\n",
      "45.15\n",
      "_____\n",
      "39.13\n",
      "_____\n",
      "51.74\n",
      "_____\n",
      "14.6\n",
      "_____\n",
      "15.1\n",
      "_____\n",
      "14.15\n",
      "_____\n",
      "25.96\n",
      "_____\n",
      "12.61\n",
      "_____\n",
      "13.76\n",
      "_____\n",
      "14.12\n",
      "_____\n",
      "10.12\n",
      "_____\n",
      "26\n",
      "_____\n",
      "50.85\n",
      "_____\n",
      "30.17\n",
      "_____\n",
      "34.7\n",
      "_____\n",
      "29.91\n",
      "_____\n",
      "-1\n",
      "_____\n",
      "49.05\n",
      "_____\n",
      "29.31\n",
      "_____\n",
      "6.318\n",
      "_____\n",
      "7.46\n",
      "_____\n",
      "9.881\n",
      "_____\n",
      "8.818\n",
      "_____\n",
      "12.38\n",
      "_____\n",
      "13.3\n",
      "_____\n",
      "8.622\n",
      "_____\n",
      "18.89\n",
      "_____\n",
      "11.59\n",
      "_____\n",
      "14.52\n",
      "_____\n",
      "12.67\n",
      "_____\n",
      "14.27\n",
      "_____\n",
      "7.055\n",
      "_____\n",
      "5.618\n",
      "_____\n",
      "8.875\n",
      "_____\n",
      "13.52\n",
      "_____\n",
      "12.41\n",
      "_____\n",
      "10.27\n",
      "_____\n",
      "7.257\n",
      "_____\n",
      "8.206\n",
      "_____\n",
      "20.51\n",
      "_____\n",
      "9.996\n",
      "_____\n",
      "10.75\n",
      "_____\n",
      "13.58\n",
      "_____\n",
      "14.14\n",
      "_____\n",
      "18.12\n",
      "_____\n",
      "13.72\n",
      "_____\n",
      "6.464\n",
      "_____\n",
      "8.047\n",
      "_____\n",
      "48.78\n",
      "_____\n",
      "7.213\n",
      "_____\n",
      "13.53\n",
      "_____\n",
      "22.77\n",
      "_____\n",
      "7.314\n",
      "_____\n",
      "7.68\n",
      "_____\n",
      "7.776\n",
      "_____\n",
      "7.776\n",
      "_____\n",
      "9.725\n",
      "_____\n",
      "10.7\n",
      "_____\n",
      "18.44\n",
      "_____\n",
      "13.11\n",
      "_____\n",
      "12.42\n",
      "_____\n",
      "43.96\n",
      "_____\n",
      "30.6\n",
      "_____\n",
      "38.62\n",
      "_____\n",
      "20.36\n",
      "_____\n",
      "7.221\n",
      "_____\n",
      "10.54\n",
      "_____\n",
      "14.96\n",
      "_____\n",
      "23.12\n",
      "_____\n",
      "3.559\n",
      "_____\n",
      "36.67\n",
      "_____\n",
      "19.07\n",
      "_____\n",
      "15.3\n",
      "_____\n",
      "17.34\n",
      "_____\n",
      "6.288\n",
      "_____\n",
      "12.89\n",
      "_____\n",
      "35.52\n",
      "_____\n",
      "39.68\n",
      "_____\n",
      "24.39\n",
      "_____\n",
      "58.07\n",
      "_____\n",
      "47.02\n",
      "_____\n",
      "23.69\n",
      "_____\n",
      "20.37\n",
      "_____\n",
      "6.767\n",
      "_____\n",
      "17.73\n",
      "_____\n",
      "29.9\n",
      "_____\n",
      "17.9\n",
      "_____\n",
      "22.83\n",
      "_____\n",
      "10.71\n",
      "_____\n",
      "21.93\n",
      "_____\n",
      "25.02\n",
      "_____\n",
      "28.18\n",
      "_____\n",
      "38.23\n",
      "_____\n",
      "42.53\n",
      "_____\n",
      "31\n",
      "_____\n",
      "20.66\n",
      "_____\n",
      "21.44\n",
      "_____\n",
      "25.67\n",
      "_____\n",
      "57.84\n",
      "_____\n",
      "14.46\n",
      "_____\n",
      "23.09\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "# post-processing;  get the z-score from profile-input (later move to SD processing...)\n",
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "with open('all_just_tompa_just_z_scores.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    with codecs.open(\"all_just_tompa_WithAllStats_withTompa_v4_rounded_annotated.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow([\"name,full_file_prefix,zscore\"])\n",
    "            else:\n",
    "                profile_input = data[1] + \"_profile_input\"\n",
    "                count = 1\n",
    "                zscore = -1\n",
    "                try:\n",
    "                    with open(profile_input, \"r+\") as f:\n",
    "                        for line in f:\n",
    "                            numbers = line.strip().split()\n",
    "                            score = numbers[-1]\n",
    "                            if (float(score) > float(zscore)):\n",
    "                                zscore = score\n",
    "                            if (count == 20):\n",
    "                                break\n",
    "                            else:\n",
    "                                count += 1\n",
    "                except OSError as e:\n",
    "                    zscore = -1\n",
    "                csvWriter.writerow(data[:2] + [zscore])\n",
    "                print(zscore)\n",
    "                print(\"_____\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
