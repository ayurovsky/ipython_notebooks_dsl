{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "#with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "\n",
    "#with codecs.open(\"Diwan_MissingArchaea_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    " #   with open('Diwan_MissingArchaea_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "with codecs.open(\"Diwan_MissingBacteria_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('Diwan_MissingBacteria_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "        \n",
    "with codecs.open(\"DiwanDataFilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('DiwanDataFilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff - just run the processing on the 48 files not found in bacteria\n",
    "# on the previous run\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_archaea_db/archaea_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingArchaea_FilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "#with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_bacteria_db/general_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        #if (name in species_dict_by_name):\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingBacteria_FilesInfo.csv', 'w') as f:\n",
    "#with open('DiwanDataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on Ruhul's 153 files\n",
    "with open('Ruhul_Annotate_153_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    with codecs.open(\"ruhul_re-annotation_all_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000191585.1_ASM19158v1_genomic.gff.gz']\n",
      "GCA_000191585.1\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000306765.2_ASM30676v2_genomic.gff.gz']\n",
      "GCA_000306765.2\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000017185.1_ASM1718v1_genomic.gff.gz']\n",
      "GCA_000017185.1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on original files from the paper\n",
    "\n",
    "#with open('Our_NOT_IN_DIWAN_SD_Bacteria_DataFilesInfo.csv', 'w') as f:\n",
    "with open('Our_NOT_IN_DIWAN_SD_Archaea_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    #with codecs.open(\"our_from_paper_not_in_diwan_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"our_from_paper_not_in_diwan_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            #names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                print(names)\n",
    "                print(prefix)\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, name missing from both bacteria and archaea:  full_file_prefix\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the 166 bad tompa entries to separate archaea from bacteria\n",
    "\n",
    "#with open('166_bad_tompa_Bacteria_DataFilesInfo.csv', 'w') as f_bacteria:\n",
    " #   with open('166_bad_tompa_Archaea_DataFilesInfo.csv', 'w') as f_archaea:\n",
    "with open('all_just_tompa_Bacteria_DataFilesInfo.csv', 'w') as f_bacteria:\n",
    "    with open('all_just_tompa_Archaea_DataFilesInfo.csv', 'w') as f_archaea:\n",
    "        csvWriter_bacteria = csv.writer(f_bacteria, delimiter=',')\n",
    "        csvWriter_bacteria.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "        csvWriter_archaea = csv.writer(f_archaea, delimiter=',')\n",
    "        csvWriter_archaea.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "        # read the large excel file, and find the correct genome\n",
    "        #with codecs.open(\"our_from_paper_not_in_diwan_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        #with codecs.open(\"166_bad_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        with codecs.open(\"all_just_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "            for row in f:\n",
    "                data = row.strip().split(\",\")\n",
    "                name = data[0]\n",
    "                full_prefix = data[1]\n",
    "                end_of_16S = data[2]\n",
    "                names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + full_prefix + '*.gz')\n",
    "                if (len(names) > 1):\n",
    "                    print(\"Error, in Bacteria, more than 1 file with prefix: \", full_prefix)\n",
    "                else:\n",
    "                    if (len(names) > 0):\n",
    "                        csvWriter_bacteria.writerow(data) #[name, full_prefix, end_of_16S, \"\"])       \n",
    "                    else:\n",
    "                        names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + full_prefix + '*.gz')\n",
    "                        if (len(names) > 1):\n",
    "                            print(\"Error, in Archaea, more than 1 file with prefix: \", full_prefix)\n",
    "                        else:\n",
    "                            if (len(names) > 0):\n",
    "                                csvWriter_archaea.writerow(data) #[name, full_prefix, end_of_16S, \"\"])\n",
    "                            else:\n",
    "                                print(\"Error, name missing from both bacteria and archaea: \", full_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post-processing - is SD that is found in the tail?\n",
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run pipeline on our stuff\n",
    "\n",
    "\n",
    "#with open('DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "#with open('Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153_rounded_annotated.csv', 'w') as f:\n",
    "#with open('Diwan_Missing_48_allstats_rounded_annotated.csv', 'w') as f:\n",
    "#with open('166_missing_DataFiles_WithAllStats_only_tompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "with open('all_just_tompa_WithAllStats_withTompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    # read the large excel file, and find the correct genome\n",
    "    #with codecs.open(\"Diwan_Missing_48_allstats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    #with codecs.open(\"Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "    #with codecs.open(\"DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    #with codecs.open(\"166_missing_allstats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"all_just_tompa_WithAllStats_withTompa_v4.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data + [\"found in 16S\"])\n",
    "                continue\n",
    "            seq = data[3]\n",
    "            complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "            reverse_complement = \"\".join(complement.get(base, base) for base in reversed(seq))\n",
    "            #print(reverse_complement)\n",
    "            if ((\"-\" not in seq) and (\"-\" not in data[2]) and (reverse_complement in data[2])):\n",
    "                csvWriter.writerow(data + [\"yes\"])\n",
    "            else:\n",
    "                csvWriter.writerow(data + [\"no\"])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.115\n",
      "TACAATC\n",
      "_____\n",
      "-1\n",
      "-------\n",
      "_____\n",
      "19.73\n",
      "CTTCTTC\n",
      "_____\n",
      "7.831\n",
      "CATTAAC\n",
      "_____\n",
      "8.775\n",
      "AAAACCA\n",
      "_____\n",
      "8.624\n",
      "CCGGCCG\n",
      "_____\n",
      "-1\n",
      "-------\n",
      "_____\n",
      "6.318\n",
      "CCGCATG\n",
      "_____\n",
      "12.38\n",
      "GGTGCAG\n",
      "_____\n",
      "18.89\n",
      "CGCGCGC\n",
      "_____\n",
      "11.59\n",
      "GATCCCG\n",
      "_____\n",
      "14.52\n",
      "CCCGCTG\n",
      "_____\n",
      "12.67\n",
      "GCTCCTC\n",
      "_____\n",
      "14.27\n",
      "CGCGCAG\n",
      "_____\n",
      "13.52\n",
      "GCCGACG\n",
      "_____\n",
      "12.41\n",
      "CTGACAC\n",
      "_____\n",
      "17.01\n",
      "GGGCGTG\n",
      "_____\n",
      "-1\n",
      "\n",
      "_____\n",
      "9.245\n",
      "CCCCGCC\n",
      "_____\n",
      "8.066\n",
      "AGCGCTA\n",
      "_____\n",
      "14.04\n",
      "GAACGAG\n",
      "_____\n",
      "10.77\n",
      "TCCTTGG\n",
      "_____\n",
      "6.767\n",
      "CGGAGCG\n",
      "_____\n",
      "14.79\n",
      "CCGGTTC\n",
      "_____\n",
      "14.2\n",
      "CCGTCGT\n",
      "_____\n",
      "12.06\n",
      "CGGGGGC\n",
      "_____\n",
      "5.053\n",
      "CGGTGCT\n",
      "_____\n",
      "22.41\n",
      "ATCTTTG\n",
      "_____\n",
      "7.804\n",
      "ACCACCA\n",
      "_____\n",
      "-1\n",
      "-------\n",
      "_____\n",
      "-1\n",
      "-------\n",
      "_____\n",
      "10.91\n",
      "AAGGAGG\n",
      "_____\n",
      "-1\n",
      "-------\n",
      "_____\n",
      "5.694\n",
      "GTATAAT\n",
      "_____\n",
      "7.715\n",
      "AGAAAAA\n",
      "_____\n",
      "8.412\n",
      "ACAAAAA\n",
      "_____\n",
      "8.492\n",
      "GGAGGGC\n",
      "_____\n",
      "6.01\n",
      "GCTACAA\n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "# post-processing;  get the z-score from profile-input (later move to SD processing...)\n",
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "#with open('all_just_tompa_just_z_scores.csv', 'w') as f:\n",
    "with open('NoSD_HighZ_withTopPattern.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    #with codecs.open(\"all_just_tompa_WithAllStats_withTompa_v4_rounded_annotated.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"NoSD_HighZ.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                #csvWriter.writerow([\"name,full_file_prefix,zscore\"])\n",
    "                csvWriter.writerow(data + [\"top pattern\"])\n",
    "            else:\n",
    "                profile_input = data[1] + \"_profile_input\"\n",
    "                count = 1\n",
    "                zscore = -1\n",
    "                zpattern = \"-------\"\n",
    "                try:\n",
    "                    with open(profile_input, \"r+\") as f:\n",
    "                        for line in f:\n",
    "                            numbers = line.strip().split()\n",
    "                            score = numbers[-1]\n",
    "                            pattern = numbers[0]\n",
    "                            if (float(score) > float(zscore)):\n",
    "                                zscore = score\n",
    "                                zpattern = pattern\n",
    "                            if (count == 20):\n",
    "                                break\n",
    "                            else:\n",
    "                                count += 1\n",
    "                except OSError as e:\n",
    "                    zscore = -1\n",
    "                    zpattern = \"-------\"\n",
    "                #csvWriter.writerow(data[:2] + [zscore])\n",
    "                csvWriter.writerow(data + [zpattern])\n",
    "                print(zscore)\n",
    "                print(zpattern)\n",
    "                print(\"_____\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n",
      "447\n",
      "[ True False  True False False  True False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False  True False  True  True False False False False\n",
      " False  True False False False False False  True False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False  True  True False False False\n",
      " False False  True False False False False  True  True False  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False  True False\n",
      "  True  True False  True  True  True  True False False False  True False\n",
      " False  True False False False  True  True False False False False  True\n",
      " False  True False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True False False  True  True  True  True False\n",
      " False False False False False False  True  True False False False False\n",
      " False  True False False False False False  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False  True False False False False\n",
      " False False False False False False False  True False False  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      "  True  True  True False  True  True False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False]\n"
     ]
    }
   ],
   "source": [
    "# process the sheet for the FDR with benjamin-hochberg\n",
    "# same package is \n",
    "# http://www.statsmodels.org/dev/generated/statsmodels.sandbox.stats.multicomp.fdrcorrection0.html#statsmodels.sandbox.stats.multicomp.fdrcorrection0\n",
    "import mne.stats\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#alpha = 0.05\n",
    "#pval = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,0.0001,0.0001,0.0001,0.0001, 0.8001, 0.8901,]\n",
    "#reject_fdr, pval_fdr = mne.stats.fdr_correction(pval, alpha=alpha, method='indep')\n",
    "#print(pval)\n",
    "#print(reject_fdr)\n",
    "#print(pval_fdr)\n",
    "\n",
    "pvals_tompa = []\n",
    "pvals_diwan = []\n",
    "with codecs.open(\"Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        if (data[0] != \"name\"):\n",
    "            if (data[7] != '-1'):\n",
    "                pvals_tompa.append(data[7])\n",
    "            pvals_diwan.append(data[11])\n",
    "print(len(pvals_tompa))\n",
    "print(len(pvals_diwan))\n",
    "pvals_tompa = np.array(pvals_tompa).astype(np.float)\n",
    "pvals_diwan = np.array(pvals_diwan).astype(np.float)\n",
    "reject_fdr_tompa, pval_fdr_tompa = mne.stats.fdr_correction(pvals_tompa, alpha=alpha, method='indep')\n",
    "reject_fdr_diwan, pval_fdr_diwan = mne.stats.fdr_correction(pvals_diwan, alpha=alpha, method='indep')\n",
    "print(reject_fdr_tompa)\n",
    "with open('Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_tompa_withFDR.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    with codecs.open(\"Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_tompa.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "        idx_tompa = 0\n",
    "        idx_diwan = 0\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data + [\"Combined P-value Accepted\", \"Combined P-value FDR Corrected\", \"Diwan P-value Accepted\", \"Diwan P-value FDR Corrected\"])\n",
    "            else:\n",
    "                append_tompa = []\n",
    "                if (data[7] != '-1'):\n",
    "                    append_tompa = [reject_fdr_tompa[idx_tompa], pval_fdr_tompa[idx_tompa]]\n",
    "                    idx_tompa += 1\n",
    "                else:\n",
    "                    append_tompa = [\"N/A\", \"-1\"]\n",
    "                append_diwan = [reject_fdr_diwan[idx_diwan], pval_fdr_diwan[idx_diwan]]\n",
    "                csvWriter.writerow(data + append_tompa + append_diwan)\n",
    "                idx_diwan += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "[False False False False False  True False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False  True False False False  True False False  True  True  True\n",
      "  True  True False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True False  True  True  True  True False False\n",
      " False False False False False False False  True False False False  True\n",
      " False  True False  True False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True  True False False False False False  True  True  True False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False False False False False False False  True  True\n",
      " False False False False  True False False  True False False False False\n",
      " False False False False False False False  True  True  True  True False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False  True False False False  True\n",
      " False False False False  True False  True False False  True  True  True\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False  True False False False False\n",
      " False  True  True False False False False  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False  True False False False False  True False\n",
      " False False  True  True  True  True  True  True  True False False False\n",
      " False False False False  True False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False  True False False False False  True\n",
      "  True  True  True False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False  True False False False False False False False\n",
      " False False  True False  True  True False  True False False False False\n",
      " False False False False False False False False  True False False]\n",
      "CPU times: user 56.2 ms, sys: 0 ns, total: 56.2 ms\n",
      "Wall time: 55.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This is only for codon pair p value\n",
    "\n",
    "# process the sheet for the FDR with benjamin-hochberg\n",
    "# same package is \n",
    "# http://www.statsmodels.org/dev/generated/statsmodels.sandbox.stats.multicomp.fdrcorrection0.html#statsmodels.sandbox.stats.multicomp.fdrcorrection0\n",
    "import codecs\n",
    "import mne.stats\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "alpha = 0.05\n",
    "#pval = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,0.0001,0.0001,0.0001,0.0001, 0.8001, 0.8901,]\n",
    "#reject_fdr, pval_fdr = mne.stats.fdr_correction(pval, alpha=alpha, method='indep')\n",
    "#print(pval)\n",
    "#print(reject_fdr)\n",
    "#print(pval_fdr)\n",
    "\n",
    "pvals_tompa = []\n",
    "with codecs.open(\"Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_original_withFDR.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        if (data[0] != \"name\"):\n",
    "            if (data[7] != '-1'):\n",
    "                pvals_tompa.append(data[5])\n",
    "print(len(pvals_tompa))\n",
    "pvals_tompa = np.array(pvals_tompa).astype(np.float)\n",
    "reject_fdr_tompa, pval_fdr_tompa = mne.stats.fdr_correction(pvals_tompa, alpha=alpha, method='indep')\n",
    "print(reject_fdr_tompa)\n",
    "with open('Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_original_with_Updated_FDR.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    with codecs.open(\"Oct_3_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats_only_original_withFDR.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "        idx_tompa = 0\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data + [\"Codon Pair P-value Accepted\", \"Codon Pair P-value FDR Corrected\"])\n",
    "            else:\n",
    "                append_tompa = []\n",
    "                if (data[7] != '-1'):\n",
    "                    append_tompa = [reject_fdr_tompa[idx_tompa], pval_fdr_tompa[idx_tompa]]\n",
    "                    idx_tompa += 1\n",
    "                else:\n",
    "                    append_tompa = [\"N/A\", \"-1\"]\n",
    "                csvWriter.writerow(data + append_tompa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burkholderia thailandensis E264: ------------- GATCACCTCCTTT\n",
      "Pseudomonas aeruginosa PAO1: ------------- GATCACCTCCTTA\n",
      "Rhodobacter sphaeroides 2.4.1: ------------- GATCACCTCCTTT\n",
      "Vibrio cholerae O1 biovar El Tor str. N16961: ------------- GATCACCTCCTTA\n",
      "Burkholderia thailandensis E264: ------------- GATCACCTCCTTT\n",
      "Pseudomonas aeruginosa PAO1: ------------- GATCACCTCCTTA\n",
      "Rhodobacter sphaeroides 2.4.1: ------------- GATCACCTCCTTT\n",
      "Vibrio cholerae O1 biovar El Tor str. N16961: ------------- GATCACCTCCTTA\n",
      "Yersinia pestis CO92: ------------- GATCACCTCCTTA\n",
      "Yersinia pestis CO92: ------------- GATCACCTCCTTA\n",
      "Corynebacterium glutamicum ATCC 13032: ------------- GATCACCTCCTTT\n",
      "Helicobacter pylori J99: ------------- GATCACCTCCTTT\n",
      "Corynebacterium glutamicum ATCC 13032: ------------- GATCACCTCCTTT\n",
      "Helicobacter pylori J99: ------------- GATCACCTCCTTT\n",
      "Bacillus subtilis subsp. subtilis str. 168: ------------- GATCACCTCCTTT\n",
      "Bacillus subtilis subsp. subtilis str. 168: ------------- GATCACCTCCTTT\n",
      "Found 830 bacteria by name\n",
      "Will update 16 bacteria\n",
      "Found 64 archaea by name\n",
      "Will update 0 archaea\n"
     ]
    }
   ],
   "source": [
    "# fix the 16S annotations in the final spreadsheet\n",
    "\n",
    "bacteria_name_to_16S_dict = dict()\n",
    "# for the bacteria; TODO:  add for archaea\n",
    "with codecs.open(\"refined_annotation_helix45_16s18_per_organism.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        if (name in bacteria_name_to_16S_dict):\n",
    "            if (data[23] != bacteria_name_to_16S_dict[name]):\n",
    "                if (bacteria_name_to_16S_dict[name] == \"-------------\"):\n",
    "                    bacteria_name_to_16S_dict[name] = data[23]\n",
    "        else:\n",
    "            bacteria_name_to_16S_dict[name] = data[23]\n",
    "            #print(name + \" \" + data[24])\n",
    "\n",
    "archaea_name_to_16S_dict = dict()\n",
    "# for the bacteria; TODO:  add for archaea\n",
    "with codecs.open(\"archaea_refined_annotation_per_organism.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        if (name in archaea_name_to_16S_dict):\n",
    "            if (data[23] != archaea_name_to_16S_dict[name]):\n",
    "                if (archaea_name_to_16S_dict[name] == \"-------------\"):\n",
    "                    archaea_name_to_16S_dict[name] = data[23]\n",
    "        else:\n",
    "            archaea_name_to_16S_dict[name] = data[23]\n",
    "            #print(name + \" \" + data[24])\n",
    "            \n",
    "\n",
    "found_bacteria = 0\n",
    "found_archaea = 0\n",
    "different_16S_bacteria = 0\n",
    "different_16S_archaea = 0\n",
    "with open('Oct_10_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    with codecs.open(\"Oct_1_Ruhul153_Plus_Diwan284_Plus_Original10_AllStats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data)\n",
    "            else:\n",
    "                name = data[0].strip()\n",
    "                new_16S = \"\"\n",
    "                if (name in bacteria_name_to_16S_dict):\n",
    "                    found_bacteria += 1\n",
    "                    if (data[2] != bacteria_name_to_16S_dict[name]):\n",
    "                        different_16S_bacteria += 1\n",
    "                        print(name + \": \" + data[2] + \" \" + bacteria_name_to_16S_dict[name])\n",
    "                        new_16S = bacteria_name_to_16S_dict[name]\n",
    "                if (name in archaea_name_to_16S_dict):\n",
    "                    found_archaea += 1\n",
    "                    if (data[2] != archaea_name_to_16S_dict[name]):\n",
    "                        different_16S_archaea += 1\n",
    "                        print(name + \": \" + data[2] + \" \" + archaea_name_to_16S_dict[name])\n",
    "                        new_16S = archaea_name_to_16S_dict[name]\n",
    "                if (new_16S != \"\"):\n",
    "                    data[2] = new_16S\n",
    "                    seq = data[3]\n",
    "                    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "                    reverse_complement = \"\".join(complement.get(base, base) for base in reversed(seq))\n",
    "                    #print(reverse_complement)\n",
    "                    if ((\"-\" not in seq) and (\"-\" not in data[2]) and (reverse_complement in data[2])):\n",
    "                        data[9] = \"yes\"\n",
    "                    else:\n",
    "                        data[9] = \"no\"\n",
    "                    csvWriter.writerow(data)\n",
    "print(\"Found \" + str(found_bacteria) + \" bacteria by name\")\n",
    "print(\"Will update \" + str(different_16S_bacteria) + \" bacteria\")\n",
    "print(\"Found \" + str(found_archaea) + \" archaea by name\")\n",
    "print(\"Will update \" + str(different_16S_archaea) + \" archaea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
