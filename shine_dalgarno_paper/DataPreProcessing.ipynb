{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "#with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "\n",
    "#with codecs.open(\"Diwan_MissingArchaea_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    " #   with open('Diwan_MissingArchaea_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "with codecs.open(\"Diwan_MissingBacteria_FilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('Diwan_MissingBacteria_FilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# get the p-value from the original diwan sheet to be attached to the 236 names\n",
    "pval_by_name = dict()\n",
    "with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        p_val = data[14]\n",
    "        #print (name + \" \" + str(p_val))\n",
    "        species_dict_by_name[name] = p_val\n",
    "        \n",
    "with codecs.open(\"DiwanDataFilesInfo.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with open('DiwanDataFilesInfo_withPvalshuffle.csv', 'w') as fout:\n",
    "        csvWriter = csv.writer(fout, delimiter=',')\n",
    "        csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\", \"pvalshuff (aff5 to highest)\"])\n",
    "        for row in f:\n",
    "            if (row.startswith(\"name\")):\n",
    "                continue\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[0].strip()\n",
    "            data.append(species_dict_by_name[name])\n",
    "            #print(data)\n",
    "            csvWriter.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff - just run the processing on the 48 files not found in bacteria\n",
    "# on the previous run\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_archaea_db/archaea_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingArchaea_FilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found matches for  0  species\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run the india paper stuff\n",
    "\n",
    "species_names = []\n",
    "species_dict_by_name = dict()\n",
    "with codecs.open(\"diwan_missing_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "#with codecs.open(\"DiwanSupplementOriginal.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        if (row.startswith(\"Species\")):\n",
    "            continue\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[0].strip()\n",
    "        diwan_anti_sd = data[8][1:-1].upper()\n",
    "        diwan_anti_sd = diwan_anti_sd.replace('U', 'T')\n",
    "        revcompl = lambda x: ''.join([{'A':'T','C':'G','G':'C','T':'A'}[B] for B in x][::-1])\n",
    "        diwan_sd = revcompl(diwan_anti_sd)\n",
    "        #if (diwan_sd != \"AGGAGG\"):\n",
    "         #   print(name, \" \", diwan_sd)\n",
    "        species_names.append(name)\n",
    "        species_dict_by_name[name] = dict()\n",
    "        species_dict_by_name[name][\"diwan_sd\"] = diwan_sd\n",
    "        species_dict_by_name[name][\"found_data_files\"] = False\n",
    "        species_dict_by_name[name][\"assebly_file_prefix\"] = \"\"\n",
    "        species_dict_by_name[name][\"16S_end\"] = \"\"\n",
    "        species_dict_by_name[name][\"original_row\"] = row\n",
    "        \n",
    "        \n",
    "# read the large excel file, and find the correct genome\n",
    "with codecs.open(\"/scratch4/moamin/ncbi_bacteria_db/general_helix45_16s18_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    for row in f:\n",
    "        data = row.strip().split(\",\")\n",
    "        name = data[3].strip()\n",
    "        #if (name in species_dict_by_name):\n",
    "        if ((name in species_dict_by_name) and (\"Complete Genome\" in data[1].strip())):\n",
    "            species_dict_by_name[name][\"16S_end\"] = data[23]\n",
    "            prefix = data[0]\n",
    "            species_dict_by_name[name][\"assebly_file_prefix\"] = prefix\n",
    "            species_dict_by_name[name][\"found_data_files\"] = True\n",
    "\n",
    "count = 0\n",
    "with open('Diwan_MissingBacteria_FilesInfo.csv', 'w') as f:\n",
    "#with open('DiwanDataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # count missing names, and get the full file path, output everything to a csv file\n",
    "    for name in species_names:\n",
    "        if (species_dict_by_name[name][\"found_data_files\"] == False):\n",
    "            #print(\"Not Found for \" + name)\n",
    "            print(species_dict_by_name[name][\"original_row\"])\n",
    "            count += 1\n",
    "        else:\n",
    "            prefix = species_dict_by_name[name][\"assebly_file_prefix\"]\n",
    "            #files = [f for f in os.listdir('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/') if re.match(r'*.gz', f)]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, species_dict_by_name[name][\"16S_end\"], species_dict_by_name[name][\"diwan_sd\"]])\n",
    "        \n",
    "# getting 236 out of 285 species (found no matches for 48 species)\n",
    "print(\"Not found matches for \", count, \" species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on Ruhul's 153 files\n",
    "with open('Ruhul_Annotate_153_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    with codecs.open(\"ruhul_re-annotation_all_organism_information.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000191585.1_ASM19158v1_genomic.gff.gz']\n",
      "GCA_000191585.1\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000306765.2_ASM30676v2_genomic.gff.gz']\n",
      "GCA_000306765.2\n",
      "['/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/GCA_000017185.1_ASM1718v1_genomic.gff.gz']\n",
      "GCA_000017185.1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# process the data file to run pipeline on original files from the paper\n",
    "\n",
    "#with open('Our_NOT_IN_DIWAN_SD_Bacteria_DataFilesInfo.csv', 'w') as f:\n",
    "with open('Our_NOT_IN_DIWAN_SD_Archaea_DataFilesInfo.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    csvWriter.writerow([\"name\",\"full_file_prefix\", \"16S_end\", \"Diwan_SD\"])\n",
    "    # read the large excel file, and find the correct genome\n",
    "    #with codecs.open(\"our_from_paper_not_in_diwan_bacteria.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(\"our_from_paper_not_in_diwan_archaea.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            name = data[3].strip()\n",
    "            prefix = data[0]\n",
    "            end_of_16S = data[23]\n",
    "            #names= glob.glob('/scratch4/moamin/ncbi_bacteria_db/GbBac_GFF/' + prefix + '*.gz')\n",
    "            names= glob.glob('/scratch4/moamin/ncbi_archaea_db/GbArchaea_GFF/' + prefix + '*.gz')\n",
    "            if (len(names) > 1):\n",
    "                print(\"Error, more than 1 file with prefix:\", first)\n",
    "            else:\n",
    "                print(names)\n",
    "                print(prefix)\n",
    "                full_prefix = os.path.split(names[0])[1]\n",
    "                full_prefix = re.sub('_genomic\\.gff\\.gz$', '', full_prefix)\n",
    "                #print(full_prefix)\n",
    "                csvWriter.writerow([name, full_prefix, end_of_16S, \"\"])       \n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing - is SD that is found in the tail?\n",
    "import csv\n",
    "import codecs\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# process the data file to run pipeline on our stuff\n",
    "\n",
    "\n",
    "#with open('DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded_annotated.csv', 'w') as f:\n",
    "#with open('Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153_rounded_annotated.csv', 'w') as f:\n",
    "with open('Diwan_Missing_48_allstats_rounded_annotated.csv', 'w') as f:\n",
    "    csvWriter = csv.writer(f, delimiter=',')\n",
    "    # read the large excel file, and find the correct genome\n",
    "    with codecs.open(\"Diwan_Missing_48_allstats.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    #with codecs.open(\"Ruhul_Annotate_153_DataFiles_WithAllStats_only_tompa_v4_153.csv\", \"r\",encoding='utf-8', errors='ignore') as f: \n",
    "    #with codecs.open(\"DiwanDataFiles_WithAllStats_236_AGGAGG_and_Tompa_v4_rounded.csv\", \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        for row in f:\n",
    "            data = row.strip().split(\",\")\n",
    "            if (data[0] == \"name\"):\n",
    "                csvWriter.writerow(data + [\"found in 16S\"])\n",
    "                continue\n",
    "            seq = data[3]\n",
    "            complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n",
    "            reverse_complement = \"\".join(complement.get(base, base) for base in reversed(seq))\n",
    "            #print(reverse_complement)\n",
    "            if ((\"-\" not in seq) and (\"-\" not in data[2]) and (reverse_complement in data[2])):\n",
    "                csvWriter.writerow(data + [\"yes\"])\n",
    "            else:\n",
    "                csvWriter.writerow(data + [\"no\"])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
